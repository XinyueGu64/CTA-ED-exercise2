install.packages(c("kableExtra","tidyverse","readr","stringr","tidytext","quanteda","textdata"))
knitr::opts_chunk$set(echo = TRUE)
library(kableExtra)
library(tidyverse) # loads dplyr, ggplot2, and others
library(readr) # more informative and easy way to import data
library(stringr) # to handle text elements
library(tidytext) # includes set of functions useful for manipulating text
library(quanteda) # includes functions to implement Lexicoder
library(textdata)
library(academictwitteR) # for fetching Twitter data
getwd()
# This is a code chunk to show the code that collected the data using the twitter API, back in 2020.
# You don't need to run this, and this chunk of code will be ignored when you knit to html, thanks to the 'eval=FALSE' command in the chunk option.
newspapers = c("TheSun", "DailyMailUK", "MetroUK", "DailyMirror",
"EveningStandard", "thetimes", "Telegraph", "guardian")
tweets <-
get_all_tweets(
users = newspapers,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-05-01T00:00:00Z",
data_path = "data/sentanalysis/",
n = Inf,
)
library(kableExtra)
library(tidyverse)
library(readr)
library(stringr)
library(tidytext)
library(quanteda)
library(textdata)
knitr::opts_chunk$set(echo = TRUE)
# go back to token element and inspect docvars
docvars(toks_news) # ok all docvars are there
tweets  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/sentanalysis/newstweets.rds?raw=true")))
head(tweets)
colnames(tweets)
tweets <- tweets %>%
select(user_username, text, created_at, user_name,
retweet_count, like_count, quote_count) %>%
rename(username = user_username,
newspaper = user_name,
tweet = text)
tweets <- tweets %>%
select(user_username, text, created_at, user_name,
retweet_count, like_count, quote_count) %>%
rename(username = user_username,
newspaper = user_name,
tweet = text)
knitr::opts_chunk$set(echo = TRUE)
```# ===== 0) 加载必要包（不要 academictwitteR）=====
knitr::opts_chunk$set(echo = TRUE)
library(kableExtra)
library(tidyverse) # loads dplyr, ggplot2, and others
library(readr) # more informative and easy way to import data
library(stringr) # to handle text elements
library(tidytext) # includes set of functions useful for manipulating text
library(quanteda) # includes functions to implement Lexicoder
library(textdata)
head(tweets)
colnames(tweets)
head(tweets)
colnames(tweets)
tweets <- tweets %>%
select(user_username, text, created_at, user_name,
retweet_count, like_count, quote_count) %>%
rename(username = user_username,
newspaper = user_name,
tweet = text)
knitr::opts_chunk$set(echo = TRUE)
library(kableExtra)
library(tidyverse) # loads dplyr, ggplot2, and others
library(readr) # more informative and easy way to import data
library(stringr) # to handle text elements
library(tidytext) # includes set of functions useful for manipulating text
library(quanteda) # includes functions to implement Lexicoder
library(textdata)
getwd()
# This is a code chunk to show the code that collected the data using the twitter API, back in 2020.
# You don't need to run this, and this chunk of code will be ignored when you knit to html, thanks to the 'eval=FALSE' command in the chunk option.
newspapers = c("TheSun", "DailyMailUK", "MetroUK", "DailyMirror",
"EveningStandard", "thetimes", "Telegraph", "guardian")
tweets <-
get_all_tweets(
users = newspapers,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-05-01T00:00:00Z",
data_path = "data/sentanalysis/",
n = Inf,
)
tweets  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/sentanalysis/newstweets.rds?raw=true")))
head(tweets)
colnames(tweets)
tweets <- tweets %>%
select(user_username, text, created_at, user_name,
retweet_count, like_count, quote_count) %>%
rename(username = user_username,
newspaper = user_name,
tweet = text)
tweets  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/sentanalysis/newstweets.rds?raw=true")))
head(tweets)
colnames(tweets)
tweets <- tweets %>%
select(user_username, text, created_at, user_name,
retweet_count, like_count, quote_count) %>%
rename(username = user_username,
newspaper = user_name,
tweet = text)
tweets <- tweets %>%
select(user_username, text, created_at, user_name,
retweet_count, like_count, quote_count) %>%
rename(username = user_username,
newspaper = user_name,
tweet = text)
tweets %>%
arrange(created_at) %>%
tail(5) %>%
kbl() %>%
kable_styling(c("striped", "hover", "condensed", "responsive"))
tidy_tweets <- tweets %>%
mutate(desc = tolower(tweet)) %>%
unnest_tokens(word, desc) %>%
filter(str_detect(word, "[a-z]"))
tidy_tweets <- tidy_tweets %>%
filter(!word %in% stop_words$word)
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
nrc_fear <- get_sentiments("nrc") %>%
filter(sentiment == "fear")
tidy_tweets %>%
inner_join(nrc_fear) %>%
count(word, sort = TRUE)
#gen data variable, order and format date
tidy_tweets$date <- as.Date(tidy_tweets$created_at)
tidy_tweets <- tidy_tweets %>%
arrange(date)
tidy_tweets$order <- 1:nrow(tidy_tweets)
#get tweet sentiment by date
tweets_nrc_sentiment <- tidy_tweets %>%
inner_join(get_sentiments("nrc")) %>%
count(date, index = order %/% 1000, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
tweets_nrc_sentiment %>%
ggplot(aes(date, sentiment)) +
geom_point(alpha=0.5) +
geom_smooth(method= loess, alpha=0.25)
tidy_tweets %>%
inner_join(get_sentiments("bing")) %>%
count(date, index = order %/% 1000, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative) %>%
ggplot(aes(date, sentiment)) +
geom_point(alpha=0.5) +
geom_smooth(method= loess, alpha=0.25) +
ylab("bing sentiment")
tidy_tweets %>%
inner_join(get_sentiments("nrc")) %>%
count(date, index = order %/% 1000, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative) %>%
ggplot(aes(date, sentiment)) +
geom_point(alpha=0.5) +
geom_smooth(method= loess, alpha=0.25) +
ylab("nrc sentiment")
tidy_tweets %>%
inner_join(get_sentiments("afinn")) %>%
group_by(date, index = order %/% 1000) %>%
summarise(sentiment = sum(value)) %>%
ggplot(aes(date, sentiment)) +
geom_point(alpha=0.5) +
geom_smooth(method= loess, alpha=0.25) +
ylab("afinn sentiment")
word <- c('death', 'illness', 'hospital', 'life', 'health',
'fatality', 'morbidity', 'deadly', 'dead', 'victim')
value <- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
mordict <- data.frame(word, value)
mordict
tidy_tweets %>%
inner_join(mordict) %>%
group_by(date, index = order %/% 1000) %>%
summarise(morwords = sum(value)) %>%
ggplot(aes(date, morwords)) +
geom_bar(stat= "identity") +
ylab("mortality words")
mordict <- c('death', 'illness', 'hospital', 'life', 'health',
'fatality', 'morbidity', 'deadly', 'dead', 'victim')
#get total tweets per day (no missing dates so no date completion required)
totals <- tidy_tweets %>%
mutate(obs=1) %>%
group_by(date) %>%
summarise(sum_words = sum(obs))
#plot
tidy_tweets %>%
mutate(obs=1) %>%
filter(grepl(paste0(mordict, collapse = "|"),word, ignore.case = T)) %>%
group_by(date) %>%
summarise(sum_mwords = sum(obs)) %>%
full_join(totals, word, by="date") %>%
mutate(sum_mwords= ifelse(is.na(sum_mwords), 0, sum_mwords),
pctmwords = sum_mwords/sum_words) %>%
ggplot(aes(date, pctmwords)) +
geom_point(alpha=0.5) +
geom_smooth(method= loess, alpha=0.25) +
xlab("Date") + ylab("% mortality words")
tweets$date <- as.Date(tweets$created_at)
tweet_corpus <- corpus(tweets, text_field = "tweet", docvars = "date")
tweets$date <- as.Date(tweets$created_at)
tweet_corpus <- corpus(tweets, text_field = "tweet")
tweets$date <- as.Date(tweets$created_at)
tweet_corpus <- corpus(tweets, text_field = "tweet")
toks_news <- tokens(tweet_corpus, remove_punct = TRUE)
# select only the "negative" and "positive" categories
data_dictionary_LSD2015_pos_neg <- data_dictionary_LSD2015[1:2]
toks_news_lsd <- tokens_lookup(toks_news, dictionary = data_dictionary_LSD2015_pos_neg)
# create a document document-feature matrix and group it by date
dfmat_news_lsd <- dfm(toks_news_lsd) %>%
dfm_group(groups = date)
# plot positive and negative valence over time
matplot(dfmat_news_lsd$date, dfmat_news_lsd, type = "l", lty = 1, col = 1:2,
ylab = "Frequency", xlab = "")
grid()
legend("topleft", col = 1:2, legend = colnames(dfmat_news_lsd), lty = 1, bg = "white")
# plot overall sentiment (positive  - negative) over time
plot(dfmat_news_lsd$date, dfmat_news_lsd[,"positive"] - dfmat_news_lsd[,"negative"],
type = "l", ylab = "Sentiment", xlab = "")
grid()
abline(h = 0, lty = 2)
negative <- dfmat_news_lsd@x[1:121]
positive <- dfmat_news_lsd@x[122:242]
date <- dfmat_news_lsd@Dimnames$docs
tidy_sent <- as.data.frame(cbind(negative, positive, date))
tidy_sent$negative <- as.numeric(tidy_sent$negative)
tidy_sent$positive <- as.numeric(tidy_sent$positive)
tidy_sent$sentiment <- tidy_sent$positive - tidy_sent$negative
tidy_sent$date <- as.Date(tidy_sent$date)
tidy_sent %>%
ggplot() +
geom_line(aes(date, sentiment))
# go back to token element and inspect docvars
docvars(toks_news) # ok all docvars are there
# look at how many different newspaper we have in the dataset
unique(docvars(toks_news)$username)
# recreate a document-feature matrix but instead of grouping it by date, we group it by 'username' (aka newspapers)
dfm_news_lsd <- dfm(toks_news_lsd) %>%
dfm_group(groups = username)
# convert it to a dataframe so it's easier to use
tidy_dfm_news_lsd <- dfm_news_lsd %>%
convert(to = "data.frame") %>%
rename("newspaper" = doc_id) %>% # when converting to data.frame, R called our grouping variable 'doc_id'. We rename it 'newspaper' instead.
mutate(sentiment = positive - negative) # create variable for overall sentiment
# plot by newspaper
tidy_dfm_news_lsd %>%
ggplot() + # when we enter ggplot environment we need to use '+' not '%>%',
geom_point(aes(x=reorder(newspaper, -sentiment), y=sentiment)) + # reordering newspaper variable so it is displayed from most negative to most positive
coord_flip() + # pivot plot by 90 degrees
xlab("Newspapers") + # label x axis
ylab("Overall tweet sentiment (negative to positive)") + # label y axis
theme_minimal() # pretty graphic theme
# recreate a document-feature matrix but instead of grouping it by date, we group it by 'username' (aka newspapers)
dfm_news_lsd <- dfm(toks_news_lsd) %>%
dfm_group(groups = interaction(username, date)) # we group by interaction variable between newspaper and date
# convert it to a dataframe so it's easier to use
tidy_dfm_news_lsd <- dfm_news_lsd %>%
convert(to = "data.frame")
head(tidy_dfm_news_lsd) # inspect
# the interaction has batched together newspaper name and date (e.g. DailyMailUK.2020-01-01).
#We want to separate them into two distinct variables. We can do it using the command extract() and regex. It's easy because the separation is always a .
tidy_dfm_news_lsd <- tidy_dfm_news_lsd %>%
extract(doc_id, into = c("newspaper", "date"), regex = "([a-zA-Z]+)\\.(.+)") %>%
mutate(date = as.Date(date)) # clarify to R that this variable is a date
head(tidy_dfm_news_lsd) # inspect
# nice! now we again have two distinct clean variables called 'newspaper' and 'date'.
tidy_dfm_news_lsd <- tidy_dfm_news_lsd %>%
mutate(sentiment = positive - negative) # recreate variable for overall sentiment
tidy_dfm_news_lsd %>%
ggplot(aes(x=date, y=sentiment)) +
geom_point(alpha=0.5) + # plot points
geom_smooth(method= loess, alpha=0.25) + # plot smooth line
facet_wrap(~newspaper) + # 'facetting' means multiplying the plots so that there is one plot for each member of the group (here, sentiment) that way you can easily compare trend across group.
xlab("date") + ylab("overall sentiment (negative to positive)") +
ggtitle("Tweet sentiment trend across 8 British newspapers") +
theme_minimal()
trans_words <- c('trans', 'transgender', 'trans rights', 'trans rights activists', 'transphobic', 'terf', 'terfs', 'transphobia', 'transphobes', 'gender critical', 'LGBTQ', 'LGBTQ+')
#get total tweets per day (no missing dates so no date completion required)
totals_newspaper <- tidy_tweets %>%
mutate(obs=1) %>%
group_by(newspaper) %>%
summarise(sum_words = sum(obs))
#plot
tidy_tweets %>%
mutate(obs=1) %>%
filter(grepl(paste0(trans_words, collapse = "|"), word, ignore.case = T)) %>%
group_by(newspaper) %>%
summarise(sum_mwords = sum(obs)) %>%
full_join(totals_newspaper, word, by="newspaper") %>%
mutate(sum_mwords= ifelse(is.na(sum_mwords), 0, sum_mwords),
pcttranswords = sum_mwords/sum_words) %>%
ggplot(aes(x=reorder(newspaper, -pcttranswords), y=pcttranswords)) +
geom_point() +
xlab("newspaper") + ylab("% words referring to trans or terfs") +
coord_flip() +
theme_minimal()
# I'm gonna create a dictionary with two categories (it could be only one but I'm feeling fancy), one for words referring to trans people, and one for words referring to transphobes/anti-trans rights
trans_dict <- dictionary(list(trans = c('trans', 'transgender', 'trans rights', 'trans rights activists', 'LGBTQ', 'LGBTQ+'),
terf = c('transphobic', 'terf', 'terfs', 'transphobia', 'transphobes', 'gender critical')))
# back to tokens object
dfm_dict_trans <- toks_news %>%
tokens_lookup(dictionary = trans_dict) %>% # look up the occurrence of my dictionaries
dfm() %>% # turn into dfm
dfm_group(groups = username) %>% # group by newspaper
convert(to = "data.frame") %>% # convert it to a dataframe
rename("newspaper" = doc_id) %>% # rename variable
full_join(totals_newspaper, word, by="newspaper")
# then just tweak the same code as before
tidy_dfm_trans <- dfm_trans %>%
dfm_group(groups = newspaper) %>% # we group by newspaper
convert(to = "data.frame") %>% # convert it to a dataframe
rename("newspaper" = doc_id) # rename variable
# I'm gonna create a dictionary with two categories (it could be only one but I'm feeling fancy), one for words referring to trans people, and one for words referring to transphobes/anti-trans rights
trans_dict <- dictionary(list(trans = c('trans', 'transgender', 'trans rights', 'trans rights activists', 'LGBTQ', 'LGBTQ+'),
terf = c('transphobic', 'terf', 'terfs', 'transphobia', 'transphobes', 'gender critical')))
# back to tokens object
dfm_dict_trans <- toks_news %>%
tokens_lookup(dictionary = trans_dict) %>% # look up the occurrence of my dictionaries
dfm() %>% # turn into dfm
dfm_group(groups = username) %>% # group by newspaper
convert(to = "data.frame") %>% # convert it to a dataframe
rename("newspaper" = doc_id) %>% # rename variable
full_join(totals_newspaper, by="newspaper")
# then just tweak the same code as before
tidy_dfm_dict_trans <- dfm_dict_trans %>%
dfm_group(groups = newspaper) %>% # we group by newspaper
convert(to = "data.frame") %>% # convert it to a dataframe
rename("newspaper" = doc_id) # rename variable
# I'm gonna create a dictionary with two categories (it could be only one but I'm feeling fancy), one for words referring to trans people, and one for words referring to transphobes/anti-trans rights
trans_dict <- dictionary(list(trans = c('trans', 'transgender', 'trans rights', 'trans rights activists', 'LGBTQ', 'LGBTQ+'),
terf = c('transphobic', 'terf', 'terfs', 'transphobia', 'transphobes', 'gender critical')))
# back to tokens object
dfm_dict_trans <- toks_news %>%
tokens_lookup(dictionary = trans_dict) %>% # look up the occurrence of my dictionaries
dfm() %>% # turn into dfm
dfm_group(groups = username) %>% # group by newspaper
convert(to = "data.frame") %>% # convert it to a dataframe
rename("newspaper" = doc_id) %>% # rename variable
full_join(totals_newspaper, by="newspaper")
# then just tweak the same code as before
# plot by newspaper
tidy_dfm_trans %>%
ggplot() + # when we enter ggplot environment we need to use '+' not '%>%',
geom_point(aes(x=reorder(newspaper, -sentiment), y=sentiment)) + # reordering newspaper variable so it is displayed from most negative to most positive
coord_flip() + # pivot plot by 90 degrees
xlab("Newspapers") + # label x axis
ylab("Overall tweet sentiment (negative to positive)") + # label y axis
theme_minimal() # pretty graphic theme
# I'm gonna create a dictionary with two categories (it could be only one but I'm feeling fancy), one for words referring to trans people, and one for words referring to transphobes/anti-trans rights
trans_dict <- dictionary(list(trans = c('trans', 'transgender', 'trans rights', 'trans rights activists', 'LGBTQ', 'LGBTQ+'),
terf = c('transphobic', 'terf', 'terfs', 'transphobia', 'transphobes', 'gender critical')))
# back to tokens object
dfm_dict_trans <- toks_news %>%
tokens_lookup(dictionary = trans_dict) %>% # look up the occurrence of my dictionaries
dfm() %>% # turn into dfm
dfm_group(groups = username) %>% # group by newspaper
convert(to = "data.frame") %>% # convert it to a dataframe
rename("newspaper" = doc_id) %>% # rename variable
full_join(totals_newspaper, by="newspaper")
# then just tweak the same code as before
# plot by newspaper (counts)
dfm_dict_trans %>%
mutate(any = trans + terf) %>%
ggplot(aes(x = reorder(newspaper, any), y = any)) +
geom_point() +
coord_flip() +
xlab("Newspapers") +
ylab("Counts of trans/terf dictionary hits") +
theme_minimal()
#plot
tidy_tweets %>%
mutate(obs=1) %>%
filter(grepl(paste0(mordict, collapse = "|"), word, ignore.case = T)) %>%
group_by(date) %>%
summarise(sum_mwords = sum(obs)) %>%
full_join(totals, word, by="date") %>%
mutate(sum_mwords= ifelse(is.na(sum_mwords), 0, sum_mwords),
pctmwords = sum_mwords/sum_words) %>%
ggplot(aes(date, pctmwords)) +
geom_point(alpha=0.5) +
geom_smooth(method= loess, alpha=0.25) +
xlab("Date") + ylab("% mortality words")
